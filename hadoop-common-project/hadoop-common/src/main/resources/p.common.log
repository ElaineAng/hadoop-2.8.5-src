T
org.apache.hadoop.nfs.NfsExports
<org.apache.hadoop.nfs.NfsExports: org.apache.hadoop.nfs.NfsExports getInstance(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"nfs.exports.allowed.hosts"	"* rw"	
T
org.apache.hadoop.nfs.NfsExports
<org.apache.hadoop.nfs.NfsExports: org.apache.hadoop.nfs.NfsExports getInstance(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"nfs.exports.cache.size"	512	
T
org.apache.hadoop.nfs.NfsExports
<org.apache.hadoop.nfs.NfsExports: org.apache.hadoop.nfs.NfsExports getInstance(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>
"nfs.exports.cache.expirytime.millis"	900000L	
T
org.apache.hadoop.util.TestNativeCrc32
<org.apache.hadoop.util.TestNativeCrc32: void setup()>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.bytes.per.checksum"	512	
T
org.apache.hadoop.fs.FileSystemTestWrapper
<org.apache.hadoop.fs.FileSystemTestWrapper: org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.file.buffer.size"	4096	
T
org.apache.hadoop.fs.FileSystemContractBaseTest
<org.apache.hadoop.fs.FileSystemContractBaseTest: void writeAndRead(org.apache.hadoop.fs.Path,byte[],int,boolean,boolean)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.file.buffer.size"	4096	
T
org.apache.hadoop.fs.contract.ContractTestUtils
<org.apache.hadoop.fs.contract.ContractTestUtils: void writeDataset(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,byte[],int,int,boolean)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.file.buffer.size"	4096	
T
org.apache.hadoop.fs.contract.ContractTestUtils
<org.apache.hadoop.fs.contract.ContractTestUtils: void createAndVerifyFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,long)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.chunk.buffer.size"	128	
T
org.apache.hadoop.fs.contract.ContractTestUtils
<org.apache.hadoop.fs.contract.ContractTestUtils: void createAndVerifyFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,long)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.chunk.modulus.size"	128	
T
org.apache.hadoop.fs.TestTrash$AuditableTrashPolicy
<org.apache.hadoop.fs.TestTrash$AuditableTrashPolicy: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)>
<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>
"fs.trash.interval"	0.0F	
T
org.apache.hadoop.fs.TestTrash$AuditableTrashPolicy
<org.apache.hadoop.fs.TestTrash$AuditableTrashPolicy: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)>
<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>
"fs.trash.interval"	0.0F	
T
org.apache.hadoop.fs.shell.TestAclCommands$StubFileSystem
<org.apache.hadoop.fs.shell.TestAclCommands$StubFileSystem: org.apache.hadoop.fs.permission.AclStatus getAclStatus(org.apache.hadoop.fs.Path)>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"stubfs.noRpcForGetAclStatus"	0	
T
org.apache.hadoop.fs.TestTrash
<org.apache.hadoop.fs.TestTrash: void trashShell(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)>
<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>
"fs.trash.interval"	0L	
T
org.apache.hadoop.conf.TestGetInstances
<org.apache.hadoop.conf.TestGetInstances: void testGetInstances()>
<org.apache.hadoop.conf.Configuration: java.util.List getInstances(java.lang.String,java.lang.Class)>
"no.such.property"	class "Lorg/apache/hadoop/conf/TestGetInstances$SampleInterface;"	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testVariableSubstitution()>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"intvar"	-1	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testVariableSubstitution()>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"my.int"	-1	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testIntegerRanges()>
<org.apache.hadoop.conf.Configuration: org.apache.hadoop.conf.Configuration$IntegerRanges getRange(java.lang.String,java.lang.String)>
"first"	null	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testIntegerRanges()>
<org.apache.hadoop.conf.Configuration: org.apache.hadoop.conf.Configuration$IntegerRanges getRange(java.lang.String,java.lang.String)>
"second"	null	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testIntegerRanges()>
<org.apache.hadoop.conf.Configuration: org.apache.hadoop.conf.Configuration$IntegerRanges getRange(java.lang.String,java.lang.String)>
"third"	null	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testGetRangeIterator()>
<org.apache.hadoop.conf.Configuration: org.apache.hadoop.conf.Configuration$IntegerRanges getRange(java.lang.String,java.lang.String)>
"Test"	""	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testGetRangeIterator()>
<org.apache.hadoop.conf.Configuration: org.apache.hadoop.conf.Configuration$IntegerRanges getRange(java.lang.String,java.lang.String)>
"Test"	"5"	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testGetRangeIterator()>
<org.apache.hadoop.conf.Configuration: org.apache.hadoop.conf.Configuration$IntegerRanges getRange(java.lang.String,java.lang.String)>
"Test"	"5-10,13-14"	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testGetRangeIterator()>
<org.apache.hadoop.conf.Configuration: org.apache.hadoop.conf.Configuration$IntegerRanges getRange(java.lang.String,java.lang.String)>
"Test"	"8-12, 5- 7"	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testHexValues()>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"test.hex1"	0	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testHexValues()>
<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>
"test.hex1"	0L	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testHexValues()>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"test.hex2"	0	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testHexValues()>
<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>
"test.hex2"	0L	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testHexValues()>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"test.hex3"	0	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testHexValues()>
<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>
"test.hex3"	0L	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testHexValues()>
<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>
"test.hex4"	0L	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testHexValues()>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"test.hex4"	0	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testIntegerValues()>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"test.int1"	0	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testIntegerValues()>
<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>
"test.int1"	0L	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testIntegerValues()>
<org.apache.hadoop.conf.Configuration: long getLongBytes(java.lang.String,long)>
"test.int1"	0L	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testIntegerValues()>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"test.int2"	0	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testIntegerValues()>
<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>
"test.int2"	0L	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testIntegerValues()>
<org.apache.hadoop.conf.Configuration: long getLongBytes(java.lang.String,long)>
"test.int2"	0L	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testIntegerValues()>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"test.int3"	0	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testIntegerValues()>
<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>
"test.int3"	0L	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testIntegerValues()>
<org.apache.hadoop.conf.Configuration: long getLongBytes(java.lang.String,long)>
"test.int3"	0L	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testIntegerValues()>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"test.int4"	0	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testIntegerValues()>
<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>
"test.int4"	0L	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testIntegerValues()>
<org.apache.hadoop.conf.Configuration: long getLongBytes(java.lang.String,long)>
"test.int4"	0L	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testIntegerValues()>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"test.int5"	0	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testHumanReadableValues()>
<org.apache.hadoop.conf.Configuration: long getLongBytes(java.lang.String,long)>
"test.humanReadableValue1"	0L	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testHumanReadableValues()>
<org.apache.hadoop.conf.Configuration: long getLongBytes(java.lang.String,long)>
"test.humanReadableValue2"	0L	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testHumanReadableValues()>
<org.apache.hadoop.conf.Configuration: long getLongBytes(java.lang.String,long)>
"test.humanReadableValue5"	0L	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testBooleanValues()>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"test.bool1"	0	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testBooleanValues()>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"test.bool2"	1	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testBooleanValues()>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"test.bool3"	0	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testBooleanValues()>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"test.bool4"	1	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testBooleanValues()>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"test.bool5"	1	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testBooleanValues()>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"test.bool6"	0	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testBooleanValues()>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"test.bool7"	1	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testBooleanValues()>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"test.bool8"	0	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testFloatValues()>
<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>
"test.float1"	0.0F	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testFloatValues()>
<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>
"test.float2"	0.0F	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testFloatValues()>
<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>
"test.float3"	0.0F	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testFloatValues()>
<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>
"test.float4"	0.0F	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testFloatValues()>
<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>
"test.float5"	0.0F	
T
org.apache.hadoop.conf.TestGetInstances
<org.apache.hadoop.conf.TestGetInstances: void testGetInstances()>
<org.apache.hadoop.conf.Configuration: java.util.List getInstances(java.lang.String,java.lang.Class)>
"empty.property"	class "Lorg/apache/hadoop/conf/TestGetInstances$SampleInterface;"	
T
org.apache.hadoop.conf.TestGetInstances
<org.apache.hadoop.conf.TestGetInstances: void testGetInstances()>
<org.apache.hadoop.conf.Configuration: java.util.List getInstances(java.lang.String,java.lang.Class)>
"some.classes"	class "Lorg/apache/hadoop/conf/TestGetInstances$SampleInterface;"	
T
org.apache.hadoop.conf.TestGetInstances
<org.apache.hadoop.conf.TestGetInstances: void testGetInstances()>
<org.apache.hadoop.conf.Configuration: java.util.List getInstances(java.lang.String,java.lang.Class)>
"some.classes"	class "Lorg/apache/hadoop/conf/TestGetInstances$SampleInterface;"	
T
org.apache.hadoop.conf.TestGetInstances
<org.apache.hadoop.conf.TestGetInstances: void testGetInstances()>
<org.apache.hadoop.conf.Configuration: java.util.List getInstances(java.lang.String,java.lang.Class)>
"some.classes"	class "Lorg/apache/hadoop/conf/TestGetInstances$SampleInterface;"	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testDoubleValues()>
<org.apache.hadoop.conf.Configuration: double getDouble(java.lang.String,double)>
"test.double1"	0.0	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testDoubleValues()>
<org.apache.hadoop.conf.Configuration: double getDouble(java.lang.String,double)>
"test.double2"	0.0	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testDoubleValues()>
<org.apache.hadoop.conf.Configuration: double getDouble(java.lang.String,double)>
"test.double3"	0.0	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testDoubleValues()>
<org.apache.hadoop.conf.Configuration: double getDouble(java.lang.String,double)>
"test.double4"	0.0	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testDoubleValues()>
<org.apache.hadoop.conf.Configuration: double getDouble(java.lang.String,double)>
"test.double5"	0.0	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testGetClass()>
<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class)>
"test.class1"	null	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testGetClass()>
<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class)>
"test.class2"	null	
T
org.apache.hadoop.security.token.delegation.TestZKDelegationTokenSecretManager
<org.apache.hadoop.security.token.delegation.TestZKDelegationTokenSecretManager: void verifyDestroy(org.apache.hadoop.security.token.delegation.web.DelegationTokenManager,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>
"zk-dt-secret-manager.zkShutdownTimeout"	10000L	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testPattern()>
<org.apache.hadoop.conf.Configuration: java.util.regex.Pattern getPattern(java.lang.String,java.util.regex.Pattern)>
"test.pattern1"	null	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testSocketAddress()>
<org.apache.hadoop.conf.Configuration: java.net.InetSocketAddress getSocketAddr(java.lang.String,java.lang.String,int)>
"myAddress"	"host:1"	2	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testSocketAddress()>
<org.apache.hadoop.conf.Configuration: java.net.InetSocketAddress getSocketAddr(java.lang.String,java.lang.String,int)>
"myAddress"	"host:1"	2	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testSocketAddress()>
<org.apache.hadoop.conf.Configuration: java.net.InetSocketAddress getSocketAddr(java.lang.String,java.lang.String,int)>
"myAddress"	"host:1"	2	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testSocketAddress()>
<org.apache.hadoop.conf.Configuration: java.net.InetSocketAddress getSocketAddr(java.lang.String,java.lang.String,int)>
"myAddress"	"host:1"	2	
T
org.apache.hadoop.conf.TestConfiguration
<org.apache.hadoop.conf.TestConfiguration: void testSocketAddress()>
<org.apache.hadoop.conf.Configuration: java.net.InetSocketAddress getSocketAddr(java.lang.String,java.lang.String,int)>
"myAddress"	"host:1"	2	
T
org.apache.hadoop.ipc.TestRPC
<org.apache.hadoop.ipc.TestRPC: void testConfRpc()>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"ipc.server.handler.queue.size"	100	
T
org.apache.hadoop.ipc.TestRPC
<org.apache.hadoop.ipc.TestRPC: void testConfRpc()>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"ipc.server.read.threadpool.size"	1	
T
org.apache.hadoop.ipc.TestIdentityProviders
<org.apache.hadoop.ipc.TestIdentityProviders: void testPluggableIdentityProvider()>
<org.apache.hadoop.conf.Configuration: java.util.List getInstances(java.lang.String,java.lang.Class)>
"identity-provider.impl"	class "Lorg/apache/hadoop/ipc/IdentityProvider;"	
F
org.apache.hadoop.ha.TestSshFenceByTcpPort
<org.apache.hadoop.ha.TestSshFenceByTcpPort: void testFence()>
<org.apache.hadoop.conf.Configuration: void set(java.lang.String,java.lang.String)>
"dfs.ha.fencing.ssh.private-key-files"	
T
org.apache.hadoop.io.TestSequenceFileSync
<org.apache.hadoop.io.TestSequenceFileSync: void testLowSyncpoint()>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.file.buffer.size"	4096	
T
org.apache.hadoop.http.TestAuthenticationSessionCookie
<org.apache.hadoop.http.TestAuthenticationSessionCookie: void startServer(boolean)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"ssl.server.keystore.type"	"jks"	
T
org.apache.hadoop.http.TestAuthenticationSessionCookie
<org.apache.hadoop.http.TestAuthenticationSessionCookie: void startServer(boolean)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"ssl.server.truststore.type"	"jks"	
T
org.apache.hadoop.http.TestSSLHttpServer
<org.apache.hadoop.http.TestSSLHttpServer: void setup()>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"ssl.server.keystore.type"	"jks"	
T
org.apache.hadoop.http.TestSSLHttpServer
<org.apache.hadoop.http.TestSSLHttpServer: void setup()>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"ssl.server.truststore.type"	"jks"	
T
org.apache.hadoop.http.TestHttpCookieFlag
<org.apache.hadoop.http.TestHttpCookieFlag: void setUp()>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"ssl.server.keystore.type"	"jks"	
T
org.apache.hadoop.http.TestHttpCookieFlag
<org.apache.hadoop.http.TestHttpCookieFlag: void setUp()>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"ssl.server.truststore.type"	"jks"	
T
org.apache.hadoop.http.TestHttpServer
<org.apache.hadoop.http.TestHttpServer: void testAuthorizationOfDefaultServlets()>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"hadoop.http.staticuser.user"	"dr.who"	
T
org.apache.hadoop.util.hash.Hash
<org.apache.hadoop.util.hash.Hash: int getHashType(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"hadoop.util.hash.type"	"murmur"	
T
org.apache.hadoop.util.JvmPauseMonitor
<org.apache.hadoop.util.JvmPauseMonitor: void <init>(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>
"jvm.pause.warn-threshold.ms"	10000L	
T
org.apache.hadoop.util.JvmPauseMonitor
<org.apache.hadoop.util.JvmPauseMonitor: void <init>(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>
"jvm.pause.info-threshold.ms"	1000L	
T
org.apache.hadoop.util.LineReader
<org.apache.hadoop.util.LineReader: void <init>(java.io.InputStream,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.file.buffer.size"	65536	
T
org.apache.hadoop.util.LineReader
<org.apache.hadoop.util.LineReader: void <init>(java.io.InputStream,org.apache.hadoop.conf.Configuration,byte[])>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.file.buffer.size"	65536	
T
org.apache.hadoop.util.NativeCodeLoader
<org.apache.hadoop.util.NativeCodeLoader: boolean getLoadNativeLibraries(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"io.native.lib.available"	1	
T
org.apache.hadoop.crypto.CryptoStreamUtils
<org.apache.hadoop.crypto.CryptoStreamUtils: int getBufferSize(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"hadoop.security.crypto.buffer.size"	8192	
T
org.apache.hadoop.crypto.key.kms.KMSClientProvider
<org.apache.hadoop.crypto.key.kms.KMSClientProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"hadoop.security.kms.client.timeout"	60	
T
org.apache.hadoop.crypto.key.kms.KMSClientProvider
<org.apache.hadoop.crypto.key.kms.KMSClientProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"hadoop.security.kms.client.authentication.retry-count"	1	
T
org.apache.hadoop.crypto.key.kms.KMSClientProvider
<org.apache.hadoop.crypto.key.kms.KMSClientProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"hadoop.security.kms.client.encrypted.key.cache.size"	500	
T
org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider
<org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider: void <init>(org.apache.hadoop.crypto.key.kms.KMSClientProvider[],long,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"hadoop.security.kms.client.failover.sleep.base.millis"	100	
T
org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider
<org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider: void <init>(org.apache.hadoop.crypto.key.kms.KMSClientProvider[],long,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"hadoop.security.kms.client.failover.sleep.max.millis"	2000	
T
org.apache.hadoop.crypto.key.KeyProvider
<org.apache.hadoop.crypto.key.KeyProvider: void <init>(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"hadoop.security.crypto.jceks.key.serialfilter"	"java.lang.Enum;java.security.KeyRep;java.security.KeyRep$Type;javax.crypto.spec.SecretKeySpec;org.apache.hadoop.crypto.key.JavaKeyStoreProvider$KeyMetadata;!*"	
T
org.apache.hadoop.crypto.key.kms.KMSClientProvider
<org.apache.hadoop.crypto.key.kms.KMSClientProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>
"hadoop.security.kms.client.encrypted.key.cache.low-watermark"	0.3F	
T
org.apache.hadoop.crypto.key.kms.KMSClientProvider
<org.apache.hadoop.crypto.key.kms.KMSClientProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"hadoop.security.kms.client.encrypted.key.cache.expiry"	43200000	
T
org.apache.hadoop.crypto.key.KeyShell$ListCommand
<org.apache.hadoop.crypto.key.KeyShell$ListCommand: boolean validate()>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"keyShell.list.metadata"	0	
T
org.apache.hadoop.crypto.key.kms.KMSClientProvider
<org.apache.hadoop.crypto.key.kms.KMSClientProvider: void <init>(java.net.URI,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"hadoop.security.kms.client.encrypted.key.cache.num.refill.threads"	2	
T
org.apache.hadoop.crypto.key.KeyProvider$Options
<org.apache.hadoop.crypto.key.KeyProvider$Options: void <init>(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"hadoop.security.key.default.cipher"	"AES/CTR/NoPadding"	
T
org.apache.hadoop.crypto.key.KeyProvider$Options
<org.apache.hadoop.crypto.key.KeyProvider$Options: void <init>(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"hadoop.security.key.default.bitlength"	128	
T
org.apache.hadoop.crypto.JceAesCtrCryptoCodec
<org.apache.hadoop.crypto.JceAesCtrCryptoCodec: void setConf(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"hadoop.security.java.secure.random.algorithm"	"SHA1PRNG"	
T
org.apache.hadoop.crypto.random.OsSecureRandom
<org.apache.hadoop.crypto.random.OsSecureRandom: void setConf(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"hadoop.security.random.device.file.path"	"/dev/urandom"	
T
org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec
<org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec: void setConf(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>
"hadoop.security.secure.random.impl"	class "Lorg/apache/hadoop/crypto/random/OsSecureRandom;"	class "Ljava/util/Random;"	
T
org.apache.hadoop.crypto.CryptoCodec
<org.apache.hadoop.crypto.CryptoCodec: org.apache.hadoop.crypto.CryptoCodec getInstance(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"hadoop.security.crypto.cipher.suite"	"AES/CTR/NoPadding"	
T
org.apache.hadoop.fs.FileSystem
<org.apache.hadoop.fs.FileSystem: java.net.URI getDefaultUri(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"fs.defaultFS"	"file:///"	
T
org.apache.hadoop.fs.FileSystem
<org.apache.hadoop.fs.FileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"fs.client.resolve.remote.symlinks"	1	
T
org.apache.hadoop.fs.ChecksumFileSystem
<org.apache.hadoop.fs.ChecksumFileSystem: void setConf(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"file.bytes-per-checksum"	512	
T
org.apache.hadoop.fs.ChecksumFileSystem
<org.apache.hadoop.fs.ChecksumFileSystem: int getSumBufferSize(int,int)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"file.stream-buffer-size"	4096	
T
org.apache.hadoop.fs.FileSystem
<org.apache.hadoop.fs.FileSystem: org.apache.hadoop.fs.FsServerDefaults getServerDefaults()>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.bytes.per.checksum"	512	
T
org.apache.hadoop.fs.FileSystem
<org.apache.hadoop.fs.FileSystem: org.apache.hadoop.fs.FsServerDefaults getServerDefaults()>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.file.buffer.size"	4096	
T
org.apache.hadoop.fs.FileSystem
<org.apache.hadoop.fs.FileSystem: org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.Path)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.file.buffer.size"	4096	
T
org.apache.hadoop.fs.FileSystem
<org.apache.hadoop.fs.FileSystem: org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path,boolean)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.file.buffer.size"	4096	
T
org.apache.hadoop.fs.FileSystem
<org.apache.hadoop.fs.FileSystem: org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.file.buffer.size"	4096	
T
org.apache.hadoop.fs.FileSystem
<org.apache.hadoop.fs.FileSystem: org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path,short)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.file.buffer.size"	4096	
T
org.apache.hadoop.fs.FileSystem
<org.apache.hadoop.fs.FileSystem: org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.Path,short,org.apache.hadoop.util.Progressable)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.file.buffer.size"	4096	
T
org.apache.hadoop.fs.FileSystem
<org.apache.hadoop.fs.FileSystem: boolean createNewFile(org.apache.hadoop.fs.Path)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.file.buffer.size"	4096	
T
org.apache.hadoop.fs.FileSystem
<org.apache.hadoop.fs.FileSystem: org.apache.hadoop.fs.FSDataOutputStream append(org.apache.hadoop.fs.Path)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.file.buffer.size"	4096	
T
org.apache.hadoop.fs.permission.FsPermission
<org.apache.hadoop.fs.permission.FsPermission: org.apache.hadoop.fs.permission.FsPermission getUMask(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"dfs.umask"	-2147483648	
T
org.apache.hadoop.fs.TrashPolicyDefault
<org.apache.hadoop.fs.TrashPolicyDefault: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)>
<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>
"fs.trash.interval"	0.0F	
T
org.apache.hadoop.fs.TrashPolicyDefault
<org.apache.hadoop.fs.TrashPolicyDefault: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)>
<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>
"fs.trash.checkpoint.interval"	0.0F	
T
org.apache.hadoop.fs.TrashPolicyDefault
<org.apache.hadoop.fs.TrashPolicyDefault: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)>
<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>
"fs.trash.interval"	0.0F	
T
org.apache.hadoop.fs.FileSystem
<org.apache.hadoop.fs.FileSystem: long getDefaultBlockSize()>
<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>
"fs.local.block.size"	33554432L	
T
org.apache.hadoop.fs.TrashPolicyDefault
<org.apache.hadoop.fs.TrashPolicyDefault: void initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)>
<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>
"fs.trash.checkpoint.interval"	0.0F	
T
org.apache.hadoop.fs.TrashPolicy
<org.apache.hadoop.fs.TrashPolicy: org.apache.hadoop.fs.TrashPolicy getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)>
<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>
"fs.trash.classname"	class "Lorg/apache/hadoop/fs/TrashPolicyDefault;"	class "Lorg/apache/hadoop/fs/TrashPolicy;"	
T
org.apache.hadoop.fs.TrashPolicy
<org.apache.hadoop.fs.TrashPolicy: org.apache.hadoop.fs.TrashPolicy getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)>
<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>
"fs.trash.classname"	class "Lorg/apache/hadoop/fs/TrashPolicyDefault;"	class "Lorg/apache/hadoop/fs/TrashPolicy;"	
T
org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker
<org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker: void <init>(org.apache.hadoop.fs.ChecksumFileSystem,org.apache.hadoop.fs.Path)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"file.stream-buffer-size"	4096	
T
org.apache.hadoop.fs.ftp.FTPFileSystem
<org.apache.hadoop.fs.ftp.FTPFileSystem: void initialize(java.net.URI,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"fs.ftp.host"	null	
T
org.apache.hadoop.fs.ftp.FTPFileSystem
<org.apache.hadoop.fs.ftp.FTPFileSystem: org.apache.commons.net.ftp.FTPClient connect()>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"fs.ftp.host.port"	21	
T
org.apache.hadoop.fs.sftp.SFTPFileSystem
<org.apache.hadoop.fs.sftp.SFTPFileSystem: void setConfigurationFromURI(java.net.URI,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"fs.sftp.host"	null	
T
org.apache.hadoop.fs.sftp.SFTPFileSystem
<org.apache.hadoop.fs.sftp.SFTPFileSystem: void setConfigurationFromURI(java.net.URI,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"fs.sftp.host.port"	22	
T
org.apache.hadoop.fs.sftp.SFTPFileSystem
<org.apache.hadoop.fs.sftp.SFTPFileSystem: void setConfigurationFromURI(java.net.URI,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"fs.sftp.connection.max"	5	
T
org.apache.hadoop.fs.sftp.SFTPFileSystem
<org.apache.hadoop.fs.sftp.SFTPFileSystem: com.jcraft.jsch.ChannelSftp connect()>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"fs.sftp.host"	null	
T
org.apache.hadoop.fs.sftp.SFTPFileSystem
<org.apache.hadoop.fs.sftp.SFTPFileSystem: com.jcraft.jsch.ChannelSftp connect()>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"fs.sftp.host.port"	22	
T
org.apache.hadoop.fs.sftp.SFTPFileSystem
<org.apache.hadoop.fs.sftp.SFTPFileSystem: com.jcraft.jsch.ChannelSftp connect()>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"fs.sftp.keyfile"	null	
T
org.apache.hadoop.fs.FileContext
<org.apache.hadoop.fs.FileContext: void <init>(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"fs.client.resolve.remote.symlinks"	1	
T
org.apache.hadoop.fs.FileContext
<org.apache.hadoop.fs.FileContext: org.apache.hadoop.fs.FileContext getFileContext(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"fs.defaultFS"	"file:///"	
T
org.apache.hadoop.fs.HarFileSystem
<org.apache.hadoop.fs.HarFileSystem: void initializeMetadataCache(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"fs.har.metadatacache.entries"	10	
T
org.apache.hadoop.fs.FileSystem$Cache
<org.apache.hadoop.fs.FileSystem$Cache: org.apache.hadoop.fs.FileSystem getInternal(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem$Cache$Key)>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"fs.automatic.close"	1	
T
org.apache.hadoop.fs.shell.FsCommand
<org.apache.hadoop.fs.shell.FsCommand: void processRawArguments(java.util.LinkedList)>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"hadoop.shell.missing.defaultFs.warning"	0	
T
org.apache.hadoop.fs.shell.Delete$Rm
<org.apache.hadoop.fs.shell.Delete$Rm: boolean canBeSafelyDeleted(org.apache.hadoop.fs.shell.PathData)>
<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>
"hadoop.shell.safely.delete.limit.num.files"	100L	
T
org.apache.hadoop.fs.GetSpaceUsed$Builder
<org.apache.hadoop.fs.GetSpaceUsed$Builder: long getInterval()>
<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>
"fs.du.interval"	600000L	
T
org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem
<org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem: org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.fs.shell.PathData,boolean,boolean)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.file.buffer.size"	4096	
T
org.apache.hadoop.fs.DF
<org.apache.hadoop.fs.DF: void <init>(java.io.File,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>
"fs.df.interval"	3000L	
T
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager
<org.apache.hadoop.security.token.delegation.web.DelegationTokenManager: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.Text)>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"zk-dt-secret-manager.enable"	0	
T
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager
<org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.Text)>
<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>
"delegation-token.update-interval.sec"	86400L	
T
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager
<org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.Text)>
<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>
"delegation-token.max-lifetime.sec"	604800L	
T
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager
<org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.Text)>
<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>
"delegation-token.renew-interval.sec"	86400L	
T
org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager
<org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.Text)>
<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>
"delegation-token.removal-scan-interval.sec"	3600L	
T
org.apache.hadoop.security.CompositeGroupsMapping
<org.apache.hadoop.security.CompositeGroupsMapping: void setConf(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"hadoop.security.group.mapping.providers.combined"	1	
T
org.apache.hadoop.conf.Configuration
<org.apache.hadoop.conf.Configuration: char[] getPasswordFromConfig(java.lang.String)>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"hadoop.security.credential.clear-text-fallback"	1	
T
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager
<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: void <init>(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>
"delegation-token.update-interval.sec"	86400L	
T
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager
<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: void <init>(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>
"delegation-token.max-lifetime.sec"	604800L	
T
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager
<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: void <init>(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>
"delegation-token.renew-interval.sec"	86400L	
T
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager
<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: void <init>(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>
"delegation-token.removal-scan-interval.sec"	3600L	
T
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager
<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: void <init>(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>
"zk-dt-secret-manager.zkShutdownTimeout"	10000L	
T
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager
<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: void <init>(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"zk-dt-secret-manager.znodeWorkingPath"	"zkdtsm"	
T
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager
<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: void <init>(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"zk-dt-secret-manager.zkSessionTimeout"	10000	
T
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager
<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: void <init>(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"zk-dt-secret-manager.zkNumRetries"	3	
T
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager
<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: void <init>(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"zk-dt-secret-manager.znodeWorkingPath"	"zkdtsm"	
T
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager
<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: void <init>(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"zk-dt-secret-manager.zkConnectionTimeout"	10000	
T
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager
<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: java.lang.String setJaasConfiguration(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"zk-dt-secret-manager.kerberos.keytab"	""	
T
org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager
<org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager: java.lang.String setJaasConfiguration(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"zk-dt-secret-manager.kerberos.principal"	""	
T
org.apache.hadoop.security.ShellBasedIdMapping
<org.apache.hadoop.security.ShellBasedIdMapping: void <init>(org.apache.hadoop.conf.Configuration,boolean)>
<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>
"usergroupid.update.millis"	900000L	
T
org.apache.hadoop.security.ShellBasedIdMapping
<org.apache.hadoop.security.ShellBasedIdMapping: void <init>(org.apache.hadoop.conf.Configuration,boolean)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"static.id.mapping.file"	"/etc/nfs.map"	
T
org.apache.hadoop.security.authorize.ProxyUsers
<org.apache.hadoop.security.authorize.ProxyUsers: org.apache.hadoop.security.authorize.ImpersonationProvider getInstance(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>
"hadoop.security.impersonation.provider.class"	class "Lorg/apache/hadoop/security/authorize/DefaultImpersonationProvider;"	class "Lorg/apache/hadoop/security/authorize/ImpersonationProvider;"	
T
org.apache.hadoop.security.UserGroupInformation
<org.apache.hadoop.security.UserGroupInformation: void initialize(org.apache.hadoop.conf.Configuration,boolean)>
<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>
"hadoop.kerberos.min.seconds.before.relogin"	60L	
T
org.apache.hadoop.security.authorize.ServiceAuthorizationManager
<org.apache.hadoop.security.authorize.ServiceAuthorizationManager: void refreshWithLoadedConfiguration(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"security.service.authorization.default.acl"	"*"	
T
org.apache.hadoop.security.authorize.ServiceAuthorizationManager
<org.apache.hadoop.security.authorize.ServiceAuthorizationManager: void refreshWithLoadedConfiguration(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"security.service.authorization.default.acl.blocked"	""	
T
org.apache.hadoop.security.KDiag
<org.apache.hadoop.security.KDiag: void validateKinitExecutable()>
<org.apache.hadoop.conf.Configuration: java.lang.String getTrimmed(java.lang.String,java.lang.String)>
"hadoop.kerberos.kinit.command"	""	
T
org.apache.hadoop.security.SecurityUtil
<org.apache.hadoop.security.SecurityUtil: boolean getLogSlowLookupsEnabled()>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"hadoop.security.dns.log-slow-lookups.enabled"	0	
T
org.apache.hadoop.security.SecurityUtil
<org.apache.hadoop.security.SecurityUtil: int getSlowLookupThresholdMs()>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"hadoop.security.dns.log-slow-lookups.threshold.ms"	1000	
T
org.apache.hadoop.security.SecurityUtil
<org.apache.hadoop.security.SecurityUtil: org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod getAuthenticationMethod(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"hadoop.security.authentication"	"simple"	
T
org.apache.hadoop.security.SecurityUtil
<org.apache.hadoop.security.SecurityUtil: void <clinit>()>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"hadoop.security.token.service.use_ip"	1	
T
org.apache.hadoop.security.SaslPropertiesResolver
<org.apache.hadoop.security.SaslPropertiesResolver: org.apache.hadoop.security.SaslPropertiesResolver getInstance(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>
"hadoop.security.saslproperties.resolver.class"	class "Lorg/apache/hadoop/security/SaslPropertiesResolver;"	class "Lorg/apache/hadoop/security/SaslPropertiesResolver;"	
T
org.apache.hadoop.security.Groups
<org.apache.hadoop.security.Groups: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Timer)>
<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>
"hadoop.security.group.mapping"	class "Lorg/apache/hadoop/security/ShellBasedUnixGroupsMapping;"	class "Lorg/apache/hadoop/security/GroupMappingServiceProvider;"	
T
org.apache.hadoop.security.Groups
<org.apache.hadoop.security.Groups: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Timer)>
<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>
"hadoop.security.groups.cache.secs"	300L	
T
org.apache.hadoop.security.Groups
<org.apache.hadoop.security.Groups: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Timer)>
<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>
"hadoop.security.groups.negative-cache.secs"	30L	
T
org.apache.hadoop.security.Groups
<org.apache.hadoop.security.Groups: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Timer)>
<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>
"hadoop.security.groups.cache.warn.after.ms"	5000L	
T
org.apache.hadoop.security.Groups
<org.apache.hadoop.security.Groups: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Timer)>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"hadoop.security.groups.cache.background.reload"	0	
T
org.apache.hadoop.security.Groups
<org.apache.hadoop.security.Groups: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Timer)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"hadoop.security.groups.cache.background.reload.threads"	3	
T
org.apache.hadoop.security.Groups
<org.apache.hadoop.security.Groups: void parseStaticMapping(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"hadoop.user.group.static.mapping.overrides"	"dr.who=;"	
T
org.apache.hadoop.security.LdapGroupsMapping
<org.apache.hadoop.security.LdapGroupsMapping: void setConf(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"hadoop.security.group.mapping.ldap.url"	""	
T
org.apache.hadoop.security.LdapGroupsMapping
<org.apache.hadoop.security.LdapGroupsMapping: void setConf(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"hadoop.security.group.mapping.ldap.bind.user"	""	
T
org.apache.hadoop.security.LdapGroupsMapping
<org.apache.hadoop.security.LdapGroupsMapping: void setConf(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"hadoop.security.group.mapping.ldap.bind.password.file"	""	
T
org.apache.hadoop.security.LdapGroupsMapping
<org.apache.hadoop.security.LdapGroupsMapping: void setConf(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"hadoop.security.group.mapping.ldap.base"	""	
T
org.apache.hadoop.security.LdapGroupsMapping
<org.apache.hadoop.security.LdapGroupsMapping: void setConf(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"hadoop.security.group.mapping.ldap.search.filter.group"	"(objectClass=group)"	
T
org.apache.hadoop.security.LdapGroupsMapping
<org.apache.hadoop.security.LdapGroupsMapping: void setConf(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"hadoop.security.group.mapping.ldap.search.filter.user"	"(&(objectClass=user)(sAMAccountName={0}))"	
T
org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory
<org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory: void init(org.apache.hadoop.security.ssl.SSLFactory$Mode)>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"hadoop.ssl.require.client.cert"	0	
T
org.apache.hadoop.security.LdapGroupsMapping
<org.apache.hadoop.security.LdapGroupsMapping: void setConf(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"hadoop.security.group.mapping.ldap.search.attr.member"	"member"	
T
org.apache.hadoop.security.LdapGroupsMapping
<org.apache.hadoop.security.LdapGroupsMapping: void setConf(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"hadoop.security.group.mapping.ldap.search.attr.group.name"	"cn"	
T
org.apache.hadoop.security.LdapGroupsMapping
<org.apache.hadoop.security.LdapGroupsMapping: void setConf(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"hadoop.security.group.mapping.ldap.posix.attr.uid.name"	"uidNumber"	
T
org.apache.hadoop.security.LdapGroupsMapping
<org.apache.hadoop.security.LdapGroupsMapping: void setConf(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"hadoop.security.group.mapping.ldap.posix.attr.gid.name"	"gidNumber"	
T
org.apache.hadoop.security.LdapGroupsMapping
<org.apache.hadoop.security.LdapGroupsMapping: void setConf(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"hadoop.security.group.mapping.ldap.directory.search.timeout"	10000	
T
org.apache.hadoop.security.ssl.SSLFactory
<org.apache.hadoop.security.ssl.SSLFactory: void <init>(org.apache.hadoop.security.ssl.SSLFactory$Mode,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"hadoop.ssl.require.client.cert"	0	
T
org.apache.hadoop.security.ssl.SSLFactory
<org.apache.hadoop.security.ssl.SSLFactory: void <init>(org.apache.hadoop.security.ssl.SSLFactory$Mode,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>
"hadoop.ssl.keystores.factory.class"	class "Lorg/apache/hadoop/security/ssl/FileBasedKeyStoresFactory;"	class "Lorg/apache/hadoop/security/ssl/KeyStoresFactory;"	
T
org.apache.hadoop.security.LdapGroupsMapping
<org.apache.hadoop.security.LdapGroupsMapping: void loadSslConf(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"hadoop.security.group.mapping.ldap.ssl.keystore"	""	
T
org.apache.hadoop.security.LdapGroupsMapping
<org.apache.hadoop.security.LdapGroupsMapping: void loadSslConf(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"hadoop.security.group.mapping.ldap.ssl.keystore.password.file"	""	
T
org.apache.hadoop.security.LdapGroupsMapping
<org.apache.hadoop.security.LdapGroupsMapping: void loadSslConf(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"hadoop.security.group.mapping.ldap.ssl.truststore"	""	
T
org.apache.hadoop.security.LdapGroupsMapping
<org.apache.hadoop.security.LdapGroupsMapping: void loadSslConf(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"hadoop.security.group.mapping.ldap.ssl.truststore.password.file"	""	
T
org.apache.hadoop.security.ssl.SSLFactory
<org.apache.hadoop.security.ssl.SSLFactory: org.apache.hadoop.conf.Configuration readSSLConfiguration(org.apache.hadoop.security.ssl.SSLFactory$Mode)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"hadoop.ssl.client.conf"	"ssl-client.xml"	
T
org.apache.hadoop.security.ssl.SSLFactory
<org.apache.hadoop.security.ssl.SSLFactory: org.apache.hadoop.conf.Configuration readSSLConfiguration(org.apache.hadoop.security.ssl.SSLFactory$Mode)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"hadoop.ssl.server.conf"	"ssl-server.xml"	
T
org.apache.hadoop.security.ssl.SSLFactory
<org.apache.hadoop.security.ssl.SSLFactory: javax.net.ssl.HostnameVerifier getHostnameVerifier(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"hadoop.ssl.hostname.verifier"	"DEFAULT"	
T
org.apache.hadoop.security.WhitelistBasedResolver
<org.apache.hadoop.security.WhitelistBasedResolver: void setConf(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"hadoop.security.sasl.fixedwhitelist.file"	"/etc/hadoop/fixedwhitelist"	
T
org.apache.hadoop.security.WhitelistBasedResolver
<org.apache.hadoop.security.WhitelistBasedResolver: void setConf(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"hadoop.security.sasl.variablewhitelist.enable"	0	
T
org.apache.hadoop.security.WhitelistBasedResolver
<org.apache.hadoop.security.WhitelistBasedResolver: void setConf(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"hadoop.security.sasl.variablewhitelist.file"	"/etc/hadoop/whitelist"	
T
org.apache.hadoop.security.WhitelistBasedResolver
<org.apache.hadoop.security.WhitelistBasedResolver: void setConf(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>
"hadoop.security.sasl.variablewhitelist.cache.secs"	3600L	
T
org.apache.hadoop.security.UserGroupInformation$1
<org.apache.hadoop.security.UserGroupInformation$1: void run()>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"hadoop.kerberos.kinit.command"	"kinit"	
T
org.apache.hadoop.ipc.Server$Listener
<org.apache.hadoop.ipc.Server$Listener: void <init>(org.apache.hadoop.ipc.Server)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"ipc.server.listen.queue.size"	128	
T
org.apache.hadoop.ipc.Client
<org.apache.hadoop.ipc.Client: int getPingInterval(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"ipc.ping.interval"	60000	
T
org.apache.hadoop.ipc.Client
<org.apache.hadoop.ipc.Client: int getTimeout(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"ipc.client.ping"	1	
T
org.apache.hadoop.ipc.Client
<org.apache.hadoop.ipc.Client: int getRpcTimeout(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"ipc.client.rpc-timeout.ms"	0	
T
org.apache.hadoop.ipc.Client
<org.apache.hadoop.ipc.Client: void <init>(java.lang.Class,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"ipc.client.connect.timeout"	20000	
T
org.apache.hadoop.ipc.Client
<org.apache.hadoop.ipc.Client: void <init>(java.lang.Class,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"ipc.client.fallback-to-simple-auth-allowed"	0	
T
org.apache.hadoop.ipc.Client
<org.apache.hadoop.ipc.Client: void <init>(java.lang.Class,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"ipc.client.async.calls.max"	100	
T
org.apache.hadoop.ipc.Client$Connection
<org.apache.hadoop.ipc.Client$Connection: void <init>(org.apache.hadoop.ipc.Client,org.apache.hadoop.ipc.Client$ConnectionId,int)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"ipc.maximum.response.length"	134217728	
T
org.apache.hadoop.ipc.Client$ConnectionId
<org.apache.hadoop.ipc.Client$ConnectionId: void <init>(java.net.InetSocketAddress,java.lang.Class,org.apache.hadoop.security.UserGroupInformation,int,org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"ipc.client.connection.maxidletime"	10000	
T
org.apache.hadoop.ipc.Client$ConnectionId
<org.apache.hadoop.ipc.Client$ConnectionId: void <init>(java.net.InetSocketAddress,java.lang.Class,org.apache.hadoop.security.UserGroupInformation,int,org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"ipc.client.connect.max.retries.on.sasl"	5	
T
org.apache.hadoop.ipc.Client$ConnectionId
<org.apache.hadoop.ipc.Client$ConnectionId: void <init>(java.net.InetSocketAddress,java.lang.Class,org.apache.hadoop.security.UserGroupInformation,int,org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"ipc.client.connect.max.retries.on.timeouts"	45	
T
org.apache.hadoop.ipc.Client$ConnectionId
<org.apache.hadoop.ipc.Client$ConnectionId: void <init>(java.net.InetSocketAddress,java.lang.Class,org.apache.hadoop.security.UserGroupInformation,int,org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"ipc.client.tcpnodelay"	1	
T
org.apache.hadoop.ipc.Client$ConnectionId
<org.apache.hadoop.ipc.Client$ConnectionId: void <init>(java.net.InetSocketAddress,java.lang.Class,org.apache.hadoop.security.UserGroupInformation,int,org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"ipc.client.low-latency"	0	
T
org.apache.hadoop.ipc.Client$ConnectionId
<org.apache.hadoop.ipc.Client$ConnectionId: void <init>(java.net.InetSocketAddress,java.lang.Class,org.apache.hadoop.security.UserGroupInformation,int,org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"ipc.client.ping"	1	
T
org.apache.hadoop.ipc.Client$ConnectionId
<org.apache.hadoop.ipc.Client$ConnectionId: org.apache.hadoop.ipc.Client$ConnectionId getConnectionId(java.net.InetSocketAddress,java.lang.Class,org.apache.hadoop.security.UserGroupInformation,int,org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"ipc.client.connect.max.retries"	10	
T
org.apache.hadoop.ipc.Client$ConnectionId
<org.apache.hadoop.ipc.Client$ConnectionId: org.apache.hadoop.ipc.Client$ConnectionId getConnectionId(java.net.InetSocketAddress,java.lang.Class,org.apache.hadoop.security.UserGroupInformation,int,org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"ipc.client.connect.retry.interval"	1000	
T
org.apache.hadoop.ipc.Server$ConnectionManager
<org.apache.hadoop.ipc.Server$ConnectionManager: void <init>(org.apache.hadoop.ipc.Server)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"ipc.client.idlethreshold"	4000	
T
org.apache.hadoop.ipc.Server$ConnectionManager
<org.apache.hadoop.ipc.Server$ConnectionManager: void <init>(org.apache.hadoop.ipc.Server)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"ipc.client.connection.idle-scan-interval.ms"	10000	
T
org.apache.hadoop.ipc.Server$ConnectionManager
<org.apache.hadoop.ipc.Server$ConnectionManager: void <init>(org.apache.hadoop.ipc.Server)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"ipc.client.connection.maxidletime"	10000	
T
org.apache.hadoop.ipc.Server$ConnectionManager
<org.apache.hadoop.ipc.Server$ConnectionManager: void <init>(org.apache.hadoop.ipc.Server)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"ipc.client.kill.max"	10	
T
org.apache.hadoop.ipc.Server$ConnectionManager
<org.apache.hadoop.ipc.Server$ConnectionManager: void <init>(org.apache.hadoop.ipc.Server)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"ipc.server.max.connections"	0	
T
org.apache.hadoop.ipc.metrics.RpcMetrics
<org.apache.hadoop.ipc.metrics.RpcMetrics: void <init>(org.apache.hadoop.ipc.Server,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"rpc.metrics.quantile.enable"	0	
T
org.apache.hadoop.ipc.DecayRpcScheduler
<org.apache.hadoop.ipc.DecayRpcScheduler: void <init>(int,java.lang.String,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"decay-scheduler.metrics.top.user.count"	10	
T
org.apache.hadoop.ipc.Server
<org.apache.hadoop.ipc.Server: void refreshCallQueue(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"ipc.server.handler.queue.size"	100	
T
org.apache.hadoop.ipc.RPC
<org.apache.hadoop.ipc.RPC: int getRpcTimeout(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"ipc.client.rpc-timeout.ms"	0	
T
org.apache.hadoop.ipc.Server
<org.apache.hadoop.ipc.Server: void <init>(java.lang.String,int,java.lang.Class,int,int,int,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.token.SecretManager,java.lang.String)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"ipc.maximum.data.length"	67108864	
T
org.apache.hadoop.ipc.Server
<org.apache.hadoop.ipc.Server: void <init>(java.lang.String,int,java.lang.Class,int,int,int,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.token.SecretManager,java.lang.String)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"ipc.server.handler.queue.size"	100	
T
org.apache.hadoop.ipc.Server
<org.apache.hadoop.ipc.Server: void <init>(java.lang.String,int,java.lang.Class,int,int,int,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.token.SecretManager,java.lang.String)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"ipc.server.max.response.size"	1048576	
T
org.apache.hadoop.ipc.Server
<org.apache.hadoop.ipc.Server: void <init>(java.lang.String,int,java.lang.Class,int,int,int,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.token.SecretManager,java.lang.String)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"ipc.server.read.threadpool.size"	1	
T
org.apache.hadoop.ipc.Server
<org.apache.hadoop.ipc.Server: void <init>(java.lang.String,int,java.lang.Class,int,int,int,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.token.SecretManager,java.lang.String)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"ipc.server.read.connection-queue.size"	100	
T
org.apache.hadoop.ipc.Server
<org.apache.hadoop.ipc.Server: void <init>(java.lang.String,int,java.lang.Class,int,int,int,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.token.SecretManager,java.lang.String)>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"hadoop.security.authorization"	0	
T
org.apache.hadoop.ipc.Server
<org.apache.hadoop.ipc.Server: void <init>(java.lang.String,int,java.lang.Class,int,int,int,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.token.SecretManager,java.lang.String)>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"ipc.server.tcpnodelay"	1	
T
org.apache.hadoop.ipc.Server
<org.apache.hadoop.ipc.Server: void <init>(java.lang.String,int,java.lang.Class,int,int,int,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.token.SecretManager,java.lang.String)>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"ipc.server.log.slow.rpc"	0	
T
org.apache.hadoop.ha.FailoverController
<org.apache.hadoop.ha.FailoverController: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.ha.HAServiceProtocol$RequestSource)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"ha.failover-controller.graceful-fence.connection.retries"	1	
T
org.apache.hadoop.ha.FailoverController
<org.apache.hadoop.ha.FailoverController: int getGracefulFenceTimeout(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"ha.failover-controller.graceful-fence.rpc-timeout.ms"	5000	
T
org.apache.hadoop.ha.FailoverController
<org.apache.hadoop.ha.FailoverController: int getRpcTimeoutToNewActive(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"ha.failover-controller.new-active.rpc-timeout.ms"	60000	
T
org.apache.hadoop.ha.HAAdmin
<org.apache.hadoop.ha.HAAdmin: void setConf(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"ha.failover-controller.cli-check.rpc-timeout.ms"	20000	
T
org.apache.hadoop.ha.ZKFCRpcServer
<org.apache.hadoop.ha.ZKFCRpcServer: void <init>(org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress,org.apache.hadoop.ha.ZKFailoverController,org.apache.hadoop.security.authorize.PolicyProvider)>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"hadoop.security.authorization"	0	
T
org.apache.hadoop.ha.ZKFailoverController
<org.apache.hadoop.ha.ZKFailoverController: void initZK()>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"ha.zookeeper.session-timeout.ms"	10000	
T
org.apache.hadoop.ha.ZKFailoverController
<org.apache.hadoop.ha.ZKFailoverController: void initZK()>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"ha.zookeeper.acl"	"world:anyone:rwcda"	
T
org.apache.hadoop.ha.ZKFailoverController
<org.apache.hadoop.ha.ZKFailoverController: void initZK()>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"ha.failover-controller.active-standby-elector.zk.op.retries"	3	
T
org.apache.hadoop.ha.ZKFailoverController
<org.apache.hadoop.ha.ZKFailoverController: java.lang.String getParentZnode()>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"ha.zookeeper.parent-znode"	"/hadoop-ha"	
T
org.apache.hadoop.ha.HealthMonitor
<org.apache.hadoop.ha.HealthMonitor: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.ha.HAServiceTarget)>
<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>
"ha.health-monitor.sleep-after-disconnect.ms"	1000L	
T
org.apache.hadoop.ha.HealthMonitor
<org.apache.hadoop.ha.HealthMonitor: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.ha.HAServiceTarget)>
<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>
"ha.health-monitor.check-interval.ms"	1000L	
T
org.apache.hadoop.ha.HealthMonitor
<org.apache.hadoop.ha.HealthMonitor: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.ha.HAServiceTarget)>
<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>
"ha.health-monitor.connect-retry-interval.ms"	1000L	
T
org.apache.hadoop.ha.HealthMonitor
<org.apache.hadoop.ha.HealthMonitor: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.ha.HAServiceTarget)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"ha.health-monitor.rpc-timeout.ms"	45000	
T
org.apache.hadoop.io.SequenceFile$BlockCompressWriter
<org.apache.hadoop.io.SequenceFile$BlockCompressWriter: void <init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Writer$Option[])>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.seqfile.compress.blocksize"	1000000	
T
org.apache.hadoop.io.BloomMapFile$Writer
<org.apache.hadoop.io.BloomMapFile$Writer: void initBloomFilter(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.mapfile.bloom.size"	1048576	
T
org.apache.hadoop.io.BloomMapFile$Writer
<org.apache.hadoop.io.BloomMapFile$Writer: void initBloomFilter(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: float getFloat(java.lang.String,float)>
"io.mapfile.bloom.error.rate"	0.005F	
T
org.apache.hadoop.io.MapFile$Reader
<org.apache.hadoop.io.MapFile$Reader: void <init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.map.index.skip"	0	
T
org.apache.hadoop.io.nativeio.NativeIO$POSIX
<org.apache.hadoop.io.nativeio.NativeIO$POSIX: void <clinit>()>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"hadoop.workaround.non.threadsafe.getpwuid"	1	
T
org.apache.hadoop.io.nativeio.NativeIO$POSIX
<org.apache.hadoop.io.nativeio.NativeIO$POSIX: void <clinit>()>
<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>
"hadoop.security.uid.cache.secs"	14400L	
T
org.apache.hadoop.io.nativeio.NativeIO
<org.apache.hadoop.io.nativeio.NativeIO: void ensureInitialized()>
<org.apache.hadoop.conf.Configuration: long getLong(java.lang.String,long)>
"hadoop.security.uid.cache.secs"	14400L	
T
org.apache.hadoop.io.compress.GzipCodec
<org.apache.hadoop.io.compress.GzipCodec: org.apache.hadoop.io.compress.CompressionOutputStream createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.file.buffer.size"	4096	
T
org.apache.hadoop.io.compress.Lz4Codec
<org.apache.hadoop.io.compress.Lz4Codec: org.apache.hadoop.io.compress.CompressionOutputStream createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.compression.codec.lz4.buffersize"	262144	
T
org.apache.hadoop.io.compress.GzipCodec
<org.apache.hadoop.io.compress.GzipCodec: org.apache.hadoop.io.compress.CompressionInputStream createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.file.buffer.size"	4096	
T
org.apache.hadoop.io.compress.SnappyCodec
<org.apache.hadoop.io.compress.SnappyCodec: org.apache.hadoop.io.compress.CompressionOutputStream createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.compression.codec.snappy.buffersize"	262144	
T
org.apache.hadoop.io.compress.Lz4Codec
<org.apache.hadoop.io.compress.Lz4Codec: org.apache.hadoop.io.compress.Compressor createCompressor()>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.compression.codec.lz4.buffersize"	262144	
T
org.apache.hadoop.io.compress.Lz4Codec
<org.apache.hadoop.io.compress.Lz4Codec: org.apache.hadoop.io.compress.Compressor createCompressor()>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"io.compression.codec.lz4.use.lz4hc"	0	
T
org.apache.hadoop.io.compress.SnappyCodec
<org.apache.hadoop.io.compress.SnappyCodec: org.apache.hadoop.io.compress.Compressor createCompressor()>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.compression.codec.snappy.buffersize"	262144	
T
org.apache.hadoop.io.compress.Lz4Codec
<org.apache.hadoop.io.compress.Lz4Codec: org.apache.hadoop.io.compress.CompressionInputStream createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.compression.codec.lz4.buffersize"	262144	
T
org.apache.hadoop.io.compress.SnappyCodec
<org.apache.hadoop.io.compress.SnappyCodec: org.apache.hadoop.io.compress.CompressionInputStream createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.compression.codec.snappy.buffersize"	262144	
T
org.apache.hadoop.io.compress.Lz4Codec
<org.apache.hadoop.io.compress.Lz4Codec: org.apache.hadoop.io.compress.Decompressor createDecompressor()>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.compression.codec.lz4.buffersize"	262144	
T
org.apache.hadoop.io.compress.SnappyCodec
<org.apache.hadoop.io.compress.SnappyCodec: org.apache.hadoop.io.compress.Decompressor createDecompressor()>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.compression.codec.snappy.buffersize"	262144	
T
org.apache.hadoop.ha.SshFenceByTcpPort
<org.apache.hadoop.ha.SshFenceByTcpPort: int getSshConnectTimeout()>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"dfs.ha.fencing.ssh.connect-timeout"	30000	
T
org.apache.hadoop.io.compress.bzip2.Bzip2Factory
<org.apache.hadoop.io.compress.bzip2.Bzip2Factory: boolean isNativeBzip2Loaded(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"io.compression.codec.bzip2.library"	"system-native"	
T
org.apache.hadoop.io.compress.bzip2.Bzip2Factory
<org.apache.hadoop.io.compress.bzip2.Bzip2Factory: boolean isNativeBzip2Loaded(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"io.native.lib.available"	1	
T
org.apache.hadoop.io.compress.bzip2.Bzip2Factory
<org.apache.hadoop.io.compress.bzip2.Bzip2Factory: int getBlockSize(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"bzip2.compress.blocksize"	9	
T
org.apache.hadoop.io.compress.bzip2.Bzip2Factory
<org.apache.hadoop.io.compress.bzip2.Bzip2Factory: int getWorkFactor(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"bzip2.compress.workfactor"	30	
T
org.apache.hadoop.io.compress.DefaultCodec
<org.apache.hadoop.io.compress.DefaultCodec: org.apache.hadoop.io.compress.CompressionOutputStream createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.file.buffer.size"	4096	
T
org.apache.hadoop.io.compress.DefaultCodec
<org.apache.hadoop.io.compress.DefaultCodec: org.apache.hadoop.io.compress.CompressionInputStream createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.file.buffer.size"	4096	
T
org.apache.hadoop.io.compress.zlib.ZlibFactory
<org.apache.hadoop.io.compress.zlib.ZlibFactory: boolean isNativeZlibLoaded(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"io.native.lib.available"	1	
T
org.apache.hadoop.io.SequenceFile
<org.apache.hadoop.io.SequenceFile: int getBufferSize(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.file.buffer.size"	4096	
T
org.apache.hadoop.io.compress.BZip2Codec
<org.apache.hadoop.io.compress.BZip2Codec: org.apache.hadoop.io.compress.CompressionOutputStream createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.file.buffer.size"	4096	
T
org.apache.hadoop.io.compress.BZip2Codec
<org.apache.hadoop.io.compress.BZip2Codec: org.apache.hadoop.io.compress.CompressionInputStream createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.file.buffer.size"	4096	
T
org.apache.hadoop.io.file.tfile.TFile
<org.apache.hadoop.io.file.tfile.TFile: int getChunkBufferSize(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"tfile.io.chunk.size"	1048576	
T
org.apache.hadoop.io.file.tfile.TFile
<org.apache.hadoop.io.file.tfile.TFile: int getFSInputBufferSize(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"tfile.fs.input.buffer.size"	262144	
T
org.apache.hadoop.io.file.tfile.TFile
<org.apache.hadoop.io.file.tfile.TFile: int getFSOutputBufferSize(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"tfile.fs.output.buffer.size"	262144	
T
org.apache.hadoop.io.SequenceFile$Sorter
<org.apache.hadoop.io.SequenceFile$Sorter: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.io.RawComparator,java.lang.Class,java.lang.Class,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Metadata)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.sort.mb"	100	
T
org.apache.hadoop.io.SequenceFile$Sorter
<org.apache.hadoop.io.SequenceFile$Sorter: void <init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.io.RawComparator,java.lang.Class,java.lang.Class,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Metadata)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.sort.factor"	100	
T
org.apache.hadoop.io.IOUtils
<org.apache.hadoop.io.IOUtils: void copyBytes(java.io.InputStream,java.io.OutputStream,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.file.buffer.size"	4096	
T
org.apache.hadoop.io.IOUtils
<org.apache.hadoop.io.IOUtils: void copyBytes(java.io.InputStream,java.io.OutputStream,org.apache.hadoop.conf.Configuration,boolean)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.file.buffer.size"	4096	
T
org.apache.hadoop.io.MapFile
<org.apache.hadoop.io.MapFile: long fix(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,boolean,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.map.index.interval"	128	
T
org.apache.hadoop.net.NetworkTopology
<org.apache.hadoop.net.NetworkTopology: org.apache.hadoop.net.NetworkTopology getInstance(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.Class getClass(java.lang.String,java.lang.Class,java.lang.Class)>
"net.topology.impl"	class "Lorg/apache/hadoop/net/NetworkTopology;"	class "Lorg/apache/hadoop/net/NetworkTopology;"	
T
org.apache.hadoop.net.NetUtils
<org.apache.hadoop.net.NetUtils: javax.net.SocketFactory getDefaultSocketFactory(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"hadoop.rpc.socket.factory.class.default"	"org.apache.hadoop.net.StandardSocketFactory"	
T
org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping
<org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping: void setConf(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"net.topology.script.number.args"	100	
T
org.apache.hadoop.net.TableMapping$RawTableMapping
<org.apache.hadoop.net.TableMapping$RawTableMapping: java.util.Map load()>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"net.topology.table.file.name"	null	
T
org.apache.hadoop.io.SequenceFile$Reader
<org.apache.hadoop.io.SequenceFile$Reader: void handleChecksumException(org.apache.hadoop.fs.ChecksumException)>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"io.skip.checksum.errors"	0	
T
org.apache.hadoop.io.SequenceFile$Reader
<org.apache.hadoop.io.SequenceFile$Reader: void handleChecksumException(org.apache.hadoop.fs.ChecksumException)>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"io.bytes.per.checksum"	512	
T
org.apache.hadoop.metrics.ganglia.GangliaContext31
<org.apache.hadoop.metrics.ganglia.GangliaContext31: void init(java.lang.String,org.apache.hadoop.metrics.ContextFactory)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"dfs.datanode.dns.interface"	"default"	
T
org.apache.hadoop.metrics.ganglia.GangliaContext31
<org.apache.hadoop.metrics.ganglia.GangliaContext31: void init(java.lang.String,org.apache.hadoop.metrics.ContextFactory)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"dfs.datanode.dns.nameserver"	"default"	
T
org.apache.hadoop.http.lib.StaticUserWebFilter
<org.apache.hadoop.http.lib.StaticUserWebFilter: java.lang.String getUsernameFromConf(org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"hadoop.http.staticuser.user"	"dr.who"	
T
org.apache.hadoop.http.HttpServer2
<org.apache.hadoop.http.HttpServer2: void initializeWebServer(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String[])>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"hadoop.http.max.threads"	-1	
T
org.apache.hadoop.http.HttpServer2
<org.apache.hadoop.http.HttpServer2: void addDefaultApps(org.mortbay.jetty.handler.ContextHandlerCollection,java.lang.String,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"hadoop.jetty.logs.serve.aliases"	1	
T
org.apache.hadoop.http.HttpServer
<org.apache.hadoop.http.HttpServer: void <init>(java.lang.String,java.lang.String,int,boolean,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.AccessControlList,org.mortbay.jetty.Connector,java.lang.String[])>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"hadoop.ssl.enabled"	0	
T
org.apache.hadoop.http.HttpServer
<org.apache.hadoop.http.HttpServer: void <init>(java.lang.String,java.lang.String,int,boolean,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.AccessControlList,org.mortbay.jetty.Connector,java.lang.String[])>
<org.apache.hadoop.conf.Configuration: int getInt(java.lang.String,int)>
"hadoop.http.max.threads"	-1	
T
org.apache.hadoop.http.HttpServer
<org.apache.hadoop.http.HttpServer: void addDefaultApps(org.mortbay.jetty.handler.ContextHandlerCollection,java.lang.String,org.apache.hadoop.conf.Configuration)>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"hadoop.jetty.logs.serve.aliases"	1	
T
org.apache.hadoop.http.HttpServer
<org.apache.hadoop.http.HttpServer: void addSslListener(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,boolean)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"ssl.server.truststore.location"	""	
T
org.apache.hadoop.http.HttpServer
<org.apache.hadoop.http.HttpServer: void addSslListener(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,boolean)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"ssl.server.truststore.password"	""	
T
org.apache.hadoop.http.HttpServer
<org.apache.hadoop.http.HttpServer: void addSslListener(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,boolean)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"ssl.server.truststore.type"	"jks"	
T
org.apache.hadoop.http.HttpServer
<org.apache.hadoop.http.HttpServer: void addSslListener(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,boolean)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"ssl.server.keystore.password"	""	
T
org.apache.hadoop.http.HttpServer
<org.apache.hadoop.http.HttpServer: void addSslListener(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,boolean)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"ssl.server.keystore.keypassword"	""	
T
org.apache.hadoop.http.HttpServer
<org.apache.hadoop.http.HttpServer: void addSslListener(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,boolean)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"ssl.server.keystore.type"	"jks"	
T
org.apache.hadoop.http.HttpServer
<org.apache.hadoop.http.HttpServer: boolean isInstrumentationAccessAllowed(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"hadoop.security.instrumentation.requires.admin"	0	
T
org.apache.hadoop.http.HttpServer
<org.apache.hadoop.http.HttpServer: boolean hasAdministratorAccess(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"hadoop.security.authorization"	0	
T
org.apache.hadoop.http.HttpServer2
<org.apache.hadoop.http.HttpServer2: boolean isStaticUserAndNoneAuthType(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest)>
<org.apache.hadoop.conf.Configuration: java.lang.String get(java.lang.String,java.lang.String)>
"hadoop.http.staticuser.user"	"dr.who"	
T
org.apache.hadoop.http.HttpServer2
<org.apache.hadoop.http.HttpServer2: boolean isInstrumentationAccessAllowed(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"hadoop.security.instrumentation.requires.admin"	0	
T
org.apache.hadoop.http.HttpServer2
<org.apache.hadoop.http.HttpServer2: boolean hasAdministratorAccess(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)>
<org.apache.hadoop.conf.Configuration: boolean getBoolean(java.lang.String,boolean)>
"hadoop.security.authorization"	0	
